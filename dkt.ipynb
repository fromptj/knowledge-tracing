{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dkt.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN7awz5DSuguYwy+pJyHgX1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","%cd drive/MyDrive/Colab Notebooks/knowledge-tracing"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZzF7pAqiNrO9","executionInfo":{"status":"ok","timestamp":1646919016195,"user_tz":-540,"elapsed":24625,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}},"outputId":"ea7c2946-4866-4d7e-883c-1a22c5143a3e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","/content/drive/MyDrive/Colab Notebooks/knowledge-tracing\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"BVDNwmhXE-eE","executionInfo":{"status":"ok","timestamp":1646919024872,"user_tz":-540,"elapsed":8681,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"outputs":[],"source":["import os\n","import argparse\n","import json\n","import pickle\n","import easydict\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","from torch.utils.data import DataLoader, random_split\n","from torch.optim import SGD, Adam\n","from torch.utils.data import Dataset\n","\n","from torch.nn import Module, Embedding, LSTM, Linear, Dropout\n","from torch.nn.functional import one_hot, binary_cross_entropy\n","from sklearn import metrics\n","from torch.nn.utils.rnn import pad_sequence"]},{"cell_type":"code","source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device, use_cuda"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VZaKH17H7JlH","executionInfo":{"status":"ok","timestamp":1646919024873,"user_tz":-540,"elapsed":17,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}},"outputId":"d172fe82-087b-494c-f4e0-53baad79b80c"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(device(type='cpu'), False)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["DATASET_DIR = \"./data/assist2009/\""],"metadata":{"id":"q-rZYJ_WZHla","executionInfo":{"status":"ok","timestamp":1646919024873,"user_tz":-540,"elapsed":14,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch, pad_val=-1):\n","    '''\n","        The collate function for torch.utils.data.DataLoader\n","        Returns:\n","            q_seqs: the question(KC) sequences with the size of \\\n","                [batch_size, maximum_sequence_length_in_the_batch]\n","            r_seqs: the response sequences with the size of \\\n","                [batch_size, maximum_sequence_length_in_the_batch]\n","            qshft_seqs: the question(KC) sequences which were shifted \\\n","                one step to the right with ths size of \\\n","                [batch_size, maximum_sequence_length_in_the_batch]\n","            rshft_seqs: the response sequences which were shifted \\\n","                one step to the right with ths size of \\\n","                [batch_size, maximum_sequence_length_in_the_batch]\n","            mask_seqs: the mask sequences indicating where \\\n","                the padded entry is with the size of \\\n","                [batch_size, maximum_sequence_length_in_the_batch]\n","    '''\n","    q_seqs = []\n","    r_seqs = []\n","    qshft_seqs = []\n","    rshft_seqs = []\n","\n","    # q_seq의 len이 n일 때, q_seq는 0부터 n-1까지, qshft_seqs(rnn output 형태)는 1부터 n까지로 cutting \n","    for q_seq, r_seq in batch:\n","        q_seqs.append(FloatTensor(q_seq[:-1]))    \n","        r_seqs.append(FloatTensor(r_seq[:-1]))\n","        qshft_seqs.append(FloatTensor(q_seq[1:]))\n","        rshft_seqs.append(FloatTensor(r_seq[1:]))\n","\n","    # maximum sequence length에 맞추어 padding 처리\n","    q_seqs = pad_sequence(\n","        q_seqs, batch_first=True, padding_value=pad_val\n","    )\n","    r_seqs = pad_sequence(\n","        r_seqs, batch_first=True, padding_value=pad_val\n","    )\n","    qshft_seqs = pad_sequence(\n","        qshft_seqs, batch_first=True, padding_value=pad_val\n","    )\n","    rshft_seqs = pad_sequence(\n","        rshft_seqs, batch_first=True, padding_value=pad_val\n","    )\n","\n","    mask_seqs = (q_seqs != pad_val) * (qshft_seqs != pad_val)\n","\n","    # [int] * [False] = 0\n","    q_seqs, r_seqs, qshft_seqs, rshft_seqs = \\\n","        q_seqs * mask_seqs, r_seqs * mask_seqs, qshft_seqs * mask_seqs, \\\n","        rshft_seqs * mask_seqs\n","\n","    return q_seqs, r_seqs, qshft_seqs, rshft_seqs, mask_seqs"],"metadata":{"id":"c7XyOfOE2uS9","executionInfo":{"status":"ok","timestamp":1646919024873,"user_tz":-540,"elapsed":13,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def match_seq_len(q_seqs, r_seqs, seq_len, pad_val=-1):\n","    '''\n","        Args:\n","            q_seqs: the question(KC) sequences with the size of \\\n","                [batch_size, some_sequence_length]\n","            r_seqs: the response sequences with the size of \\\n","                [batch_size, some_sequence_length]\n","            Note that the \"some_sequence_length\" is not uniform over \\\n","                the whole batch of q_seqs and r_seqs\n","            seq_len: the sequence length to match the q_seqs, r_seqs \\\n","                to same length\n","            pad_val: the padding value for the sequence with the length \\\n","                longer than seq_len\n","        Returns:\n","            proc_q_seqs: the processed q_seqs with the size of \\\n","                [batch_size, seq_len + 1]\n","            proc_r_seqs: the processed r_seqs with the size of \\\n","                [batch_size, seq_len + 1]\n","    '''\n","    proc_q_seqs = []\n","    proc_r_seqs = []\n","\n","\n","    # [0, 1, 2, ..., 9]를 [0, 1, 2, 3, 4], [1, 2, 3, 4, 5] 식으로 index를 +1하면서 분할하는 게 아니라 [0, 1, 2, 3, 4], [5, 6, 7, 8, 9]\n","    for q_seq, r_seq in zip(q_seqs, r_seqs):\n","        i = 0\n","        while i + seq_len + 1 < len(q_seq):\n","            # 본래 seq_len보다 +1 해서 만듦 ( 마지막 index는 label로 처리해야 하므로 )\n","            proc_q_seqs.append(q_seq[i:i + seq_len + 1])\n","            proc_r_seqs.append(r_seq[i:i + seq_len + 1])\n","\n","            i += seq_len + 1\n","\n","        proc_q_seqs.append(\n","            np.concatenate(\n","                [\n","                    q_seq[i:],\n","                    np.array([pad_val] * (i + seq_len + 1 - len(q_seq)))\n","                ]\n","            )\n","        )\n","        proc_r_seqs.append(\n","            np.concatenate(\n","                [\n","                    r_seq[i:],\n","                    np.array([pad_val] * (i + seq_len + 1 - len(q_seq)))\n","                ]\n","            )\n","        )\n","\n","    return proc_q_seqs, proc_r_seqs"],"metadata":{"id":"TyFBEpINcBdi","executionInfo":{"status":"ok","timestamp":1646919024873,"user_tz":-540,"elapsed":13,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class ASSIST2009(Dataset):\n","    def __init__(self, seq_len, dataset_dir=DATASET_DIR) -> None:\n","        super().__init__()\n","\n","        self.dataset_dir = dataset_dir\n","        self.dataset_path = os.path.join(\n","            self.dataset_dir, \"skill_builder_data.csv\"\n","        )\n","\n","        if os.path.exists(os.path.join(self.dataset_dir, \"q_seqs.pkl\")):\n","            with open(os.path.join(self.dataset_dir, \"q_seqs.pkl\"), \"rb\") as f:\n","                self.q_seqs = pickle.load(f)\n","            with open(os.path.join(self.dataset_dir, \"r_seqs.pkl\"), \"rb\") as f:\n","                self.r_seqs = pickle.load(f)\n","            with open(os.path.join(self.dataset_dir, \"q_list.pkl\"), \"rb\") as f:\n","                self.q_list = pickle.load(f)\n","            with open(os.path.join(self.dataset_dir, \"u_list.pkl\"), \"rb\") as f:\n","                self.u_list = pickle.load(f)\n","            with open(os.path.join(self.dataset_dir, \"q2idx.pkl\"), \"rb\") as f:\n","                self.q2idx = pickle.load(f)\n","            with open(os.path.join(self.dataset_dir, \"u2idx.pkl\"), \"rb\") as f:\n","                self.u2idx = pickle.load(f)\n","        else:\n","            self.q_seqs, self.r_seqs, self.q_list, self.u_list, self.q2idx, \\\n","                self.u2idx = self.preprocess()\n","\n","        self.num_u = self.u_list.shape[0]\n","        self.num_q = self.q_list.shape[0]\n","\n","        if seq_len:\n","            self.q_seqs, self.r_seqs = \\\n","                match_seq_len(self.q_seqs, self.r_seqs, seq_len)\n","\n","        self.len = len(self.q_seqs)\n","\n","    def __getitem__(self, index):\n","        return self.q_seqs[index], self.r_seqs[index]\n","\n","    def __len__(self):\n","        return self.len\n","\n","    def preprocess(self):\n","        df = pd.read_csv(self.dataset_path, encoding=\"ISO-8859-1\").dropna(subset=[\"skill_name\"])\\\n","            .drop_duplicates(subset=[\"order_id\", \"skill_name\"])\\\n","            .sort_values(by=[\"order_id\"])\n","\n","        u_list = np.unique(df[\"user_id\"].values)\n","        q_list = np.unique(df[\"skill_name\"].values)\n","\n","        u2idx = {u: idx for idx, u in enumerate(u_list)}\n","        q2idx = {q: idx for idx, q in enumerate(q_list)}\n","\n","        q_seqs = []\n","        r_seqs = []\n","\n","        for u in u_list:\n","            df_u = df[df[\"user_id\"] == u]\n","\n","            q_seq = np.array([q2idx[q] for q in df_u[\"skill_name\"]])\n","            r_seq = df_u[\"correct\"].values\n","\n","            q_seqs.append(q_seq)\n","            r_seqs.append(r_seq)\n","\n","        with open(os.path.join(self.dataset_dir, \"q_seqs.pkl\"), \"wb\") as f:\n","            pickle.dump(q_seqs, f)\n","        with open(os.path.join(self.dataset_dir, \"r_seqs.pkl\"), \"wb\") as f:\n","            pickle.dump(r_seqs, f)\n","        with open(os.path.join(self.dataset_dir, \"q_list.pkl\"), \"wb\") as f:\n","            pickle.dump(q_list, f)\n","        with open(os.path.join(self.dataset_dir, \"u_list.pkl\"), \"wb\") as f:\n","            pickle.dump(u_list, f)\n","        with open(os.path.join(self.dataset_dir, \"q2idx.pkl\"), \"wb\") as f:\n","            pickle.dump(q2idx, f)\n","        with open(os.path.join(self.dataset_dir, \"u2idx.pkl\"), \"wb\") as f:\n","            pickle.dump(u2idx, f)\n","\n","        return q_seqs, r_seqs, q_list, u_list, q2idx, u2idx"],"metadata":{"id":"IrShEGvnY8EI","executionInfo":{"status":"ok","timestamp":1646919024874,"user_tz":-540,"elapsed":14,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    from torch.cuda import FloatTensor\n","    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n","else:\n","    from torch import FloatTensor"],"metadata":{"id":"9H4cbWs8hwoU","executionInfo":{"status":"ok","timestamp":1646919024874,"user_tz":-540,"elapsed":13,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["args = easydict.EasyDict(\n","    {\n","        \"batch_size\" : 256,\n","        \"num_epochs\" : 10,\n","        \"train_ratio\" : 0.9,\n","        \"learning_rate\" : 0.001,\n","        \"optimizer\" : \"adam\",\n","        \"seq_len\" : 100,\n","        \"emb_size\" : 100,\n","        \"hidden_size\" : 100 ,\n","        \"gpu_id\" : 0\n","    })\n","\n","# device = torch.device(\"cuda:\" + str(args.gpu_id) if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"1yDsvjUB0G7y","executionInfo":{"status":"ok","timestamp":1646919024875,"user_tz":-540,"elapsed":14,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class DKT(Module):\n","    '''\n","        Args:\n","            num_q: the total number of the questions(KCs) in the given dataset\n","            emb_size: the dimension of the embedding vectors in this model\n","            hidden_size: the dimension of the hidden vectors in this model\n","    '''\n","    \n","    def __init__(self, num_q, emb_size, hidden_size):\n","        super().__init__()\n","        self.num_q = num_q\n","        self.emb_size = emb_size\n","        self.hidden_size = hidden_size\n","\n","        self.interaction_emb = Embedding(self.num_q * 2, self.emb_size)\n","        self.lstm_layer = LSTM(\n","            self.emb_size, self.hidden_size, batch_first=True\n","        )\n","        self.out_layer = Linear(self.hidden_size, self.num_q)\n","        self.dropout_layer = Dropout()\n","\n","    def forward(self, q, r):\n","        '''\n","            Args:\n","                q: the question(KC) sequence with the size of [batch_size, n]\n","                r: the response sequence with the size of [batch_size, n]\n","            Returns:\n","                y: the knowledge level about the all questions(KCs), [batch_size, seq_len, hidden_size]\n","        '''\n","        x = q + self.num_q * r\n","\n","        h, _ = self.lstm_layer(self.interaction_emb(x))\n","        y = self.out_layer(h)\n","        y = self.dropout_layer(y)\n","        y = torch.sigmoid(y)\n","\n","        return y\n","\n","    def train_model(\n","        self, train_loader, test_loader, num_epochs, optimizer, ckpt_path\n","    ):\n","        '''\n","            Args:\n","                train_loader: the PyTorch DataLoader instance for training\n","                test_loader: the PyTorch DataLoader instance for test\n","                num_epochs: the number of epochs\n","                opt: the optimization to train this model\n","                ckpt_path: the path to save this model's parameters\n","        '''\n","        aucs = []\n","        loss_means = []\n","\n","        max_auc = 0\n","\n","        for i in range(1, num_epochs + 1):\n","            loss_mean = []\n","\n","            for data in train_loader:\n","                q, r, qshft, rshft, m = data\n","\n","                self.train()\n","\n","                y = self(q.long(), r.long())\n","                # 각 시점에서 다음 문제의 index만 제하고 0 처리, 그 다음 차원 통합\n","                y = (y * one_hot(qshft.long(), self.num_q)).sum(-1) \n","\n","                y = torch.masked_select(y, m)\n","\n","                t = torch.masked_select(rshft, m)\n","\n","                optimizer.zero_grad()\n","                loss = binary_cross_entropy(y, t)\n","                loss.backward()\n","                optimizer.step()\n","\n","                loss_mean.append(loss.detach().cpu().numpy())\n","\n","            with torch.no_grad():\n","                for data in test_loader:\n","                    q, r, qshft, rshft, m = data\n","\n","                    self.eval()\n","\n","                    y = self(q.long(), r.long())\n","\n","                    y = (y * one_hot(qshft.long(), self.num_q)).sum(-1)   \n","\n","                    y = torch.masked_select(y, m).detach().cpu()\n","                    t = torch.masked_select(rshft, m).detach().cpu()\n","\n","                    auc = metrics.roc_auc_score(\n","                        y_true=t.numpy(), y_score=y.numpy()\n","                    )\n","\n","                    loss_mean = np.mean(loss_mean)\n","\n","                    print(\n","                        \"Epoch: {},   AUC: {},   Loss Mean: {}\"\n","                        .format(i, auc, loss_mean)\n","                    )\n","\n","                    if auc > max_auc:\n","                        torch.save(\n","                            self.state_dict(),\n","                            os.path.join(\n","                                ckpt_path, \"model.ckpt\"\n","                            )\n","                        )\n","                        max_auc = auc\n","\n","                    aucs.append(auc)\n","                    loss_means.append(loss_mean)\n","\n","        return aucs, loss_means"],"metadata":{"id":"-480DfL6JwoW","executionInfo":{"status":"ok","timestamp":1646919025278,"user_tz":-540,"elapsed":417,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["dataset = ASSIST2009(5)"],"metadata":{"id":"ucGbzSvGjmn6","executionInfo":{"status":"ok","timestamp":1646919029784,"user_tz":-540,"elapsed":4508,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["model = DKT(dataset.num_q, args.emb_size, args.hidden_size)"],"metadata":{"id":"nD8MCnX4zk_5","executionInfo":{"status":"ok","timestamp":1646919029785,"user_tz":-540,"elapsed":5,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["train_size = int(len(dataset) * args.train_ratio)\n","test_size = len(dataset) - train_size\n","\n","\n","# train_dataset : [ train_size, seq_len ] ( match_seq_len을 통해 패딩처리되어 있음 )\n","# test_dataset : [ test_size, seq_len ]\n","train_dataset, test_dataset = random_split(\n","    dataset, [train_size, test_size]\n",")"],"metadata":{"id":"A-8TNaEW1tao","executionInfo":{"status":"ok","timestamp":1646919029785,"user_tz":-540,"elapsed":4,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["if os.path.exists(os.path.join(dataset.dataset_dir, \"train_indices.pkl\")):\n","    with open(os.path.join(dataset.dataset_dir, \"train_indices.pkl\"), \"rb\") as f:\n","        train_dataset.indices = pickle.load(f)\n","    with open(os.path.join(dataset.dataset_dir, \"test_indices.pkl\"), \"rb\") as f:\n","        test_dataset.indices = pickle.load(f)\n","else:\n","    with open(os.path.join(dataset.dataset_dir, \"train_indices.pkl\"), \"wb\") as f:\n","        pickle.dump(train_dataset.indices, f)\n","    with open(os.path.join(dataset.dataset_dir, \"test_indices.pkl\"), \"wb\") as f:\n","        pickle.dump(test_dataset.indices, f)"],"metadata":{"id":"EqOaiLzo2FSu","executionInfo":{"status":"ok","timestamp":1646919031326,"user_tz":-540,"elapsed":1544,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(\n","    train_dataset, batch_size=args.batch_size, shuffle=True,\n","    collate_fn=collate_fn\n",")\n","test_loader = DataLoader(\n","    test_dataset, batch_size=test_size, shuffle=True,\n","    collate_fn=collate_fn\n",")"],"metadata":{"id":"JWkQyI7s2iEJ","executionInfo":{"status":"ok","timestamp":1646919031327,"user_tz":-540,"elapsed":4,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["optimizer = Adam(model.parameters(), args.learning_rate)\n","modelname = \"DKT\" + \\\n","  \"_bs_\" + str(args.batch_size) + \\\n","  \"_nemb_\" + str(args.emb_size) + \\\n","  \"_lr_\" + str(args.learning_rate)"],"metadata":{"id":"FueDn82Z3KpC","executionInfo":{"status":"ok","timestamp":1646919031327,"user_tz":-540,"elapsed":3,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["aucs, loss_means = model.train_model(\n","        train_loader, test_loader, args.num_epochs, optimizer, \"./ckpts/\"\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R9SUIoqt3mfk","executionInfo":{"status":"ok","timestamp":1646919134815,"user_tz":-540,"elapsed":103491,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}},"outputId":"228c1878-e41e-484b-8170-783ef9b0ae66"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1,   AUC: 0.7866887261053133,   Loss Mean: 0.6227133274078369\n","Epoch: 2,   AUC: 0.7954699291222699,   Loss Mean: 0.6022433042526245\n","Epoch: 3,   AUC: 0.7990790640814379,   Loss Mean: 0.5994665622711182\n","Epoch: 4,   AUC: 0.8006111990559164,   Loss Mean: 0.5951282978057861\n","Epoch: 5,   AUC: 0.8010540372706851,   Loss Mean: 0.5930237174034119\n","Epoch: 6,   AUC: 0.8006461955373816,   Loss Mean: 0.5913102030754089\n","Epoch: 7,   AUC: 0.8011501052057696,   Loss Mean: 0.5902721285820007\n","Epoch: 8,   AUC: 0.8018882441583015,   Loss Mean: 0.5888826847076416\n","Epoch: 9,   AUC: 0.8015807663536223,   Loss Mean: 0.5885145664215088\n","Epoch: 10,   AUC: 0.8022189349735289,   Loss Mean: 0.586394727230072\n"]}]}]}