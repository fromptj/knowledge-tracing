{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dkt.ipynb","provenance":[],"authorship_tag":"ABX9TyPQ2lEdLrZgrv+fdw/xjfiP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","%cd drive/MyDrive/Colab Notebooks/knowledge-tracing"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZzF7pAqiNrO9","executionInfo":{"status":"ok","timestamp":1646571627050,"user_tz":-540,"elapsed":2873,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}},"outputId":"1e3ae515-d47a-4873-d328-7cf70637df7f"},"execution_count":125,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","[Errno 2] No such file or directory: 'drive/MyDrive/Colab Notebooks/knowledge-tracing'\n","/content/drive/MyDrive/Colab Notebooks/knowledge-tracing\n"]}]},{"cell_type":"code","execution_count":126,"metadata":{"id":"BVDNwmhXE-eE","executionInfo":{"status":"ok","timestamp":1646571627051,"user_tz":-540,"elapsed":18,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"outputs":[],"source":["import os\n","import argparse\n","import json\n","import pickle\n","import easydict\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","from torch.utils.data import DataLoader, random_split\n","from torch.optim import SGD, Adam\n","from torch.utils.data import Dataset\n","\n","from torch.nn import Module, Embedding, LSTM, Linear, Dropout\n","from torch.nn.functional import one_hot, binary_cross_entropy\n","from sklearn import metrics\n","from torch.nn.utils.rnn import pad_sequence"]},{"cell_type":"code","source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device, use_cuda"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VZaKH17H7JlH","executionInfo":{"status":"ok","timestamp":1646571627051,"user_tz":-540,"elapsed":17,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}},"outputId":"b7476345-0996-417d-e4b2-bbb19b25ab0c"},"execution_count":127,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(device(type='cpu'), False)"]},"metadata":{},"execution_count":127}]},{"cell_type":"code","source":["DATASET_DIR = \"./data/assist2009/\""],"metadata":{"id":"q-rZYJ_WZHla","executionInfo":{"status":"ok","timestamp":1646571627052,"user_tz":-540,"elapsed":15,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":128,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch, pad_val=-1):\n","    '''\n","        The collate function for torch.utils.data.DataLoader\n","        Returns:\n","            q_seqs: the question(KC) sequences with the size of \\\n","                [batch_size, maximum_sequence_length_in_the_batch]\n","            r_seqs: the response sequences with the size of \\\n","                [batch_size, maximum_sequence_length_in_the_batch]\n","            qshft_seqs: the question(KC) sequences which were shifted \\\n","                one step to the right with ths size of \\\n","                [batch_size, maximum_sequence_length_in_the_batch]\n","            rshft_seqs: the response sequences which were shifted \\\n","                one step to the right with ths size of \\\n","                [batch_size, maximum_sequence_length_in_the_batch]\n","            mask_seqs: the mask sequences indicating where \\\n","                the padded entry is with the size of \\\n","                [batch_size, maximum_sequence_length_in_the_batch]\n","    '''\n","    q_seqs = []\n","    r_seqs = []\n","    qshft_seqs = []\n","    rshft_seqs = []\n","\n","    # q_seq의 len이 n일 때, q_seq는 0부터 n-1까지, qshft_seqs(rnn output 형태)는 1부터 n까지로 cutting \n","    for q_seq, r_seq in batch:\n","        q_seqs.append(FloatTensor(q_seq[:-1]))    \n","        r_seqs.append(FloatTensor(r_seq[:-1]))\n","        qshft_seqs.append(FloatTensor(q_seq[1:]))\n","        rshft_seqs.append(FloatTensor(r_seq[1:]))\n","\n","    # maximum sequence length에 맞추어 padding 처리\n","    q_seqs = pad_sequence(\n","        q_seqs, batch_first=True, padding_value=pad_val\n","    )\n","    r_seqs = pad_sequence(\n","        r_seqs, batch_first=True, padding_value=pad_val\n","    )\n","    qshft_seqs = pad_sequence(\n","        qshft_seqs, batch_first=True, padding_value=pad_val\n","    )\n","    rshft_seqs = pad_sequence(\n","        rshft_seqs, batch_first=True, padding_value=pad_val\n","    )\n","\n","    mask_seqs = (q_seqs != pad_val) * (qshft_seqs != pad_val)\n","\n","    # [int] * [False] = 0\n","    q_seqs, r_seqs, qshft_seqs, rshft_seqs = \\\n","        q_seqs * mask_seqs, r_seqs * mask_seqs, qshft_seqs * mask_seqs, \\\n","        rshft_seqs * mask_seqs\n","\n","    return q_seqs, r_seqs, qshft_seqs, rshft_seqs, mask_seqs"],"metadata":{"id":"c7XyOfOE2uS9","executionInfo":{"status":"ok","timestamp":1646571627052,"user_tz":-540,"elapsed":15,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":129,"outputs":[]},{"cell_type":"code","source":["def match_seq_len(q_seqs, r_seqs, seq_len, pad_val=-1):\n","    '''\n","        Args:\n","            q_seqs: the question(KC) sequences with the size of \\\n","                [batch_size, some_sequence_length]\n","            r_seqs: the response sequences with the size of \\\n","                [batch_size, some_sequence_length]\n","            Note that the \"some_sequence_length\" is not uniform over \\\n","                the whole batch of q_seqs and r_seqs\n","            seq_len: the sequence length to match the q_seqs, r_seqs \\\n","                to same length\n","            pad_val: the padding value for the sequence with the length \\\n","                longer than seq_len\n","        Returns:\n","            proc_q_seqs: the processed q_seqs with the size of \\\n","                [batch_size, seq_len + 1]\n","            proc_r_seqs: the processed r_seqs with the size of \\\n","                [batch_size, seq_len + 1]\n","    '''\n","    proc_q_seqs = []\n","    proc_r_seqs = []\n","\n","\n","    # [0, 1, 2, ..., 9]를 [0, 1, 2, 3, 4], [1, 2, 3, 4, 5] 식으로 index를 +1하면서 분할하는 게 아니라 [0, 1, 2, 3, 4], [5, 6, 7, 8, 9]\n","    for q_seq, r_seq in zip(q_seqs, r_seqs):\n","        i = 0\n","        while i + seq_len + 1 < len(q_seq):\n","            # 본래 seq_len보다 +1 해서 만듦 ( 마지막 index는 label로 처리해야 하므로 )\n","            proc_q_seqs.append(q_seq[i:i + seq_len + 1])\n","            proc_r_seqs.append(r_seq[i:i + seq_len + 1])\n","\n","            i += seq_len + 1\n","\n","        proc_q_seqs.append(\n","            np.concatenate(\n","                [\n","                    q_seq[i:],\n","                    np.array([pad_val] * (i + seq_len + 1 - len(q_seq)))\n","                ]\n","            )\n","        )\n","        proc_r_seqs.append(\n","            np.concatenate(\n","                [\n","                    r_seq[i:],\n","                    np.array([pad_val] * (i + seq_len + 1 - len(q_seq)))\n","                ]\n","            )\n","        )\n","\n","    return proc_q_seqs, proc_r_seqs"],"metadata":{"id":"TyFBEpINcBdi","executionInfo":{"status":"ok","timestamp":1646571627053,"user_tz":-540,"elapsed":16,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":130,"outputs":[]},{"cell_type":"code","source":["class ASSIST2009(Dataset):\n","    def __init__(self, seq_len, dataset_dir=DATASET_DIR) -> None:\n","        super().__init__()\n","\n","        self.dataset_dir = dataset_dir\n","        self.dataset_path = os.path.join(\n","            self.dataset_dir, \"skill_builder_data.csv\"\n","        )\n","\n","        if os.path.exists(os.path.join(self.dataset_dir, \"q_seqs.pkl\")):\n","            with open(os.path.join(self.dataset_dir, \"q_seqs.pkl\"), \"rb\") as f:\n","                self.q_seqs = pickle.load(f)\n","            with open(os.path.join(self.dataset_dir, \"r_seqs.pkl\"), \"rb\") as f:\n","                self.r_seqs = pickle.load(f)\n","            with open(os.path.join(self.dataset_dir, \"q_list.pkl\"), \"rb\") as f:\n","                self.q_list = pickle.load(f)\n","            with open(os.path.join(self.dataset_dir, \"u_list.pkl\"), \"rb\") as f:\n","                self.u_list = pickle.load(f)\n","            with open(os.path.join(self.dataset_dir, \"q2idx.pkl\"), \"rb\") as f:\n","                self.q2idx = pickle.load(f)\n","            with open(os.path.join(self.dataset_dir, \"u2idx.pkl\"), \"rb\") as f:\n","                self.u2idx = pickle.load(f)\n","        else:\n","            self.q_seqs, self.r_seqs, self.q_list, self.u_list, self.q2idx, \\\n","                self.u2idx = self.preprocess()\n","\n","        self.num_u = self.u_list.shape[0]\n","        self.num_q = self.q_list.shape[0]\n","\n","        if seq_len:\n","            self.q_seqs, self.r_seqs = \\\n","                match_seq_len(self.q_seqs, self.r_seqs, seq_len)\n","\n","        self.len = len(self.q_seqs)\n","\n","    def __getitem__(self, index):\n","        return self.q_seqs[index], self.r_seqs[index]\n","\n","    def __len__(self):\n","        return self.len\n","\n","    def preprocess(self):\n","        df = pd.read_csv(self.dataset_path).dropna(subset=[\"skill_name\"])\\\n","            .drop_duplicates(subset=[\"order_id\", \"skill_name\"])\\\n","            .sort_values(by=[\"order_id\"])\n","\n","        u_list = np.unique(df[\"user_id\"].values)\n","        q_list = np.unique(df[\"skill_name\"].values)\n","\n","        u2idx = {u: idx for idx, u in enumerate(u_list)}\n","        q2idx = {q: idx for idx, q in enumerate(q_list)}\n","\n","        q_seqs = []\n","        r_seqs = []\n","\n","        for u in u_list:\n","            df_u = df[df[\"user_id\"] == u]\n","\n","            q_seq = np.array([q2idx[q] for q in df_u[\"skill_name\"]])\n","            r_seq = df_u[\"correct\"].values\n","\n","            q_seqs.append(q_seq)\n","            r_seqs.append(r_seq)\n","\n","        with open(os.path.join(self.dataset_dir, \"q_seqs.pkl\"), \"wb\") as f:\n","            pickle.dump(q_seqs, f)\n","        with open(os.path.join(self.dataset_dir, \"r_seqs.pkl\"), \"wb\") as f:\n","            pickle.dump(r_seqs, f)\n","        with open(os.path.join(self.dataset_dir, \"q_list.pkl\"), \"wb\") as f:\n","            pickle.dump(q_list, f)\n","        with open(os.path.join(self.dataset_dir, \"u_list.pkl\"), \"wb\") as f:\n","            pickle.dump(u_list, f)\n","        with open(os.path.join(self.dataset_dir, \"q2idx.pkl\"), \"wb\") as f:\n","            pickle.dump(q2idx, f)\n","        with open(os.path.join(self.dataset_dir, \"u2idx.pkl\"), \"wb\") as f:\n","            pickle.dump(u2idx, f)\n","\n","        return q_seqs, r_seqs, q_list, u_list, q2idx, u2idx"],"metadata":{"id":"IrShEGvnY8EI","executionInfo":{"status":"ok","timestamp":1646571627053,"user_tz":-540,"elapsed":16,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":131,"outputs":[]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    from torch.cuda import FloatTensor\n","    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n","else:\n","    from torch import FloatTensor"],"metadata":{"id":"9H4cbWs8hwoU","executionInfo":{"status":"ok","timestamp":1646571627054,"user_tz":-540,"elapsed":17,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":132,"outputs":[]},{"cell_type":"code","source":["args = easydict.EasyDict(\n","    {\n","        \"batch_size\" : 256,\n","        \"num_epochs\" : 100,\n","        \"train_ratio\" : 0.9,\n","        \"learning_rate\" : 0.001,\n","        \"optimizer\" : \"adam\",\n","        \"seq_len\" : 100,\n","        \"emb_size\" : 100,\n","        \"hidden_size\" : 100 ,\n","        \"gpu_id\" : 0\n","    })\n","\n","# device = torch.device(\"cuda:\" + str(args.gpu_id) if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"1yDsvjUB0G7y","executionInfo":{"status":"ok","timestamp":1646571627054,"user_tz":-540,"elapsed":16,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":133,"outputs":[]},{"cell_type":"code","source":["class DKT(Module):\n","    '''\n","        Args:\n","            num_q: the total number of the questions(KCs) in the given dataset\n","            emb_size: the dimension of the embedding vectors in this model\n","            hidden_size: the dimension of the hidden vectors in this model\n","    '''\n","    \n","    def __init__(self, num_q, emb_size, hidden_size):\n","        super().__init__()\n","        self.num_q = num_q\n","        self.emb_size = emb_size\n","        self.hidden_size = hidden_size\n","\n","        self.interaction_emb = Embedding(self.num_q * 2, self.emb_size)\n","        self.lstm_layer = LSTM(\n","            self.emb_size, self.hidden_size, batch_first=True\n","        )\n","        self.out_layer = Linear(self.hidden_size, self.num_q)\n","        self.dropout_layer = Dropout()\n","\n","    def forward(self, q, r):\n","        '''\n","            Args:\n","                q: the question(KC) sequence with the size of [batch_size, n]\n","                r: the response sequence with the size of [batch_size, n]\n","            Returns:\n","                y: the knowledge level about the all questions(KCs), [batch_size, seq_len, hidden_size]\n","        '''\n","        x = q + self.num_q * r\n","\n","        h, _ = self.lstm_layer(self.interaction_emb(x))\n","        y = self.out_layer(h)\n","        y = self.dropout_layer(y)\n","        y = torch.sigmoid(y)\n","\n","        return y\n","\n","    def train_model(\n","        self, train_loader, test_loader, num_epochs, optimizer, ckpt_path\n","    ):\n","        '''\n","            Args:\n","                train_loader: the PyTorch DataLoader instance for training\n","                test_loader: the PyTorch DataLoader instance for test\n","                num_epochs: the number of epochs\n","                opt: the optimization to train this model\n","                ckpt_path: the path to save this model's parameters\n","        '''\n","        aucs = []\n","        loss_means = []\n","\n","        max_auc = 0\n","\n","        for i in range(1, num_epochs + 1):\n","            loss_mean = []\n","\n","            for data in train_loader:\n","                q, r, qshft, rshft, m = data\n","\n","                self.train()\n","\n","                y = self(q.long(), r.long())\n","                # 각 시점에서 다음 문제의 index만 제하고 0 처리, 그 다음 차원 통합\n","                y = (y * one_hot(qshft.long(), self.num_q)).sum(-1) \n","\n","                y = torch.masked_select(y, m)\n","\n","                t = torch.masked_select(rshft, m)\n","\n","                optimizer.zero_grad()\n","                loss = binary_cross_entropy(y, t)\n","                loss.backward()\n","                optimizer.step()\n","\n","                loss_mean.append(loss.detach().cpu().numpy())\n","\n","            with torch.no_grad():\n","                for data in test_loader:\n","                    q, r, qshft, rshft, m = data\n","\n","                    self.eval()\n","\n","                    y = self(q.long(), r.long())\n","\n","                    y = (y * one_hot(qshft.long(), self.num_q)).sum(-1)   \n","\n","                    y = torch.masked_select(y, m).detach().cpu()\n","                    t = torch.masked_select(rshft, m).detach().cpu()\n","\n","                    auc = metrics.roc_auc_score(\n","                        y_true=t.numpy(), y_score=y.numpy()\n","                    )\n","\n","                    loss_mean = np.mean(loss_mean)\n","\n","                    print(\n","                        \"Epoch: {},   AUC: {},   Loss Mean: {}\"\n","                        .format(i, auc, loss_mean)\n","                    )\n","\n","                    if auc > max_auc:\n","                        torch.save(\n","                            self.state_dict(),\n","                            os.path.join(\n","                                ckpt_path, \"model.ckpt\"\n","                            )\n","                        )\n","                        max_auc = auc\n","\n","                    aucs.append(auc)\n","                    loss_means.append(loss_mean)\n","\n","        return aucs, loss_means"],"metadata":{"id":"-480DfL6JwoW","executionInfo":{"status":"ok","timestamp":1646571627054,"user_tz":-540,"elapsed":16,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":134,"outputs":[]},{"cell_type":"code","source":["dataset = ASSIST2009(5)"],"metadata":{"id":"ucGbzSvGjmn6","executionInfo":{"status":"ok","timestamp":1646571627540,"user_tz":-540,"elapsed":502,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":135,"outputs":[]},{"cell_type":"code","source":["model = DKT(dataset.num_q, args.emb_size, args.hidden_size)"],"metadata":{"id":"nD8MCnX4zk_5","executionInfo":{"status":"ok","timestamp":1646571627540,"user_tz":-540,"elapsed":7,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":136,"outputs":[]},{"cell_type":"code","source":["train_size = int(len(dataset) * args.train_ratio)\n","test_size = len(dataset) - train_size\n","\n","\n","# train_dataset : [ train_size, seq_len ] ( match_seq_len을 통해 패딩처리되어 있음 )\n","# test_dataset : [ test_size, seq_len ]\n","train_dataset, test_dataset = random_split(\n","    dataset, [train_size, test_size]\n",")"],"metadata":{"id":"A-8TNaEW1tao","executionInfo":{"status":"ok","timestamp":1646571627541,"user_tz":-540,"elapsed":7,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":137,"outputs":[]},{"cell_type":"code","source":["if os.path.exists(os.path.join(dataset.dataset_dir, \"train_indices.pkl\")):\n","    with open(os.path.join(dataset.dataset_dir, \"train_indices.pkl\"), \"rb\") as f:\n","        train_dataset.indices = pickle.load(f)\n","    with open(os.path.join(dataset.dataset_dir, \"test_indices.pkl\"), \"rb\") as f:\n","        test_dataset.indices = pickle.load(f)\n","else:\n","    with open(os.path.join(dataset.dataset_dir, \"train_indices.pkl\"), \"wb\") as f:\n","        pickle.dump(train_dataset.indices, f)\n","    with open(os.path.join(dataset.dataset_dir, \"test_indices.pkl\"), \"wb\") as f:\n","        pickle.dump(test_dataset.indices, f)"],"metadata":{"id":"EqOaiLzo2FSu","executionInfo":{"status":"ok","timestamp":1646571627542,"user_tz":-540,"elapsed":8,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":138,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(\n","    train_dataset, batch_size=args.batch_size, shuffle=True,\n","    collate_fn=collate_fn\n",")\n","test_loader = DataLoader(\n","    test_dataset, batch_size=test_size, shuffle=True,\n","    collate_fn=collate_fn\n",")"],"metadata":{"id":"JWkQyI7s2iEJ","executionInfo":{"status":"ok","timestamp":1646571627542,"user_tz":-540,"elapsed":7,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":139,"outputs":[]},{"cell_type":"code","source":["optimizer = Adam(model.parameters(), args.learning_rate)\n","modelname = \"DKT\" + \\\n","  \"_bs_\" + str(args.batch_size) + \\\n","  \"_nemb_\" + str(args.emb_size) + \\\n","  \"_lr_\" + str(args.learning_rate)"],"metadata":{"id":"FueDn82Z3KpC","executionInfo":{"status":"ok","timestamp":1646571627543,"user_tz":-540,"elapsed":8,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":140,"outputs":[]},{"cell_type":"code","source":["aucs, loss_means = model.train_model(\n","        train_loader, test_loader, args.num_epochs, optimizer, \"./ckpts/\"\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R9SUIoqt3mfk","executionInfo":{"status":"ok","timestamp":1646572560852,"user_tz":-540,"elapsed":933317,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}},"outputId":"b4373c8a-d94a-4886-b799-36b67eb81684"},"execution_count":141,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1,   AUC: 0.8318545750671711,   Loss Mean: 0.5762569308280945\n","Epoch: 2,   AUC: 0.8344678860947314,   Loss Mean: 0.5654698014259338\n","Epoch: 3,   AUC: 0.8352928775484851,   Loss Mean: 0.5645772218704224\n","Epoch: 4,   AUC: 0.8358397759691611,   Loss Mean: 0.5644055008888245\n","Epoch: 5,   AUC: 0.8365924257962871,   Loss Mean: 0.5627449154853821\n","Epoch: 6,   AUC: 0.8368972495108519,   Loss Mean: 0.5627660155296326\n","Epoch: 7,   AUC: 0.8360421796868274,   Loss Mean: 0.5626019239425659\n","Epoch: 8,   AUC: 0.8370978672102612,   Loss Mean: 0.5628410577774048\n","Epoch: 9,   AUC: 0.8371254689919188,   Loss Mean: 0.561762809753418\n","Epoch: 10,   AUC: 0.8376170727080144,   Loss Mean: 0.5608963370323181\n","Epoch: 11,   AUC: 0.8384426811180258,   Loss Mean: 0.56232088804245\n","Epoch: 12,   AUC: 0.8378215095769839,   Loss Mean: 0.5611234307289124\n","Epoch: 13,   AUC: 0.8374773864499019,   Loss Mean: 0.5609697699546814\n","Epoch: 14,   AUC: 0.8378897480939558,   Loss Mean: 0.5610206723213196\n","Epoch: 15,   AUC: 0.8371545220229547,   Loss Mean: 0.5600491166114807\n","Epoch: 16,   AUC: 0.8375837360375087,   Loss Mean: 0.5606204271316528\n","Epoch: 17,   AUC: 0.8374731939516973,   Loss Mean: 0.5603736639022827\n","Epoch: 18,   AUC: 0.8380775674564548,   Loss Mean: 0.5599930882453918\n","Epoch: 19,   AUC: 0.8376445307669076,   Loss Mean: 0.559542179107666\n","Epoch: 20,   AUC: 0.8372692793922868,   Loss Mean: 0.5592701435089111\n","Epoch: 21,   AUC: 0.8370024580799256,   Loss Mean: 0.5585482120513916\n","Epoch: 22,   AUC: 0.8375744694246294,   Loss Mean: 0.5589776039123535\n","Epoch: 23,   AUC: 0.8370595703516622,   Loss Mean: 0.5582702159881592\n","Epoch: 24,   AUC: 0.8367302419057443,   Loss Mean: 0.5589421391487122\n","Epoch: 25,   AUC: 0.8360059703137475,   Loss Mean: 0.5585926175117493\n","Epoch: 26,   AUC: 0.8365288249675468,   Loss Mean: 0.5591288208961487\n","Epoch: 27,   AUC: 0.8360612878036442,   Loss Mean: 0.5583025813102722\n","Epoch: 28,   AUC: 0.8357643180123464,   Loss Mean: 0.5573942065238953\n","Epoch: 29,   AUC: 0.8367111916285767,   Loss Mean: 0.5578100085258484\n","Epoch: 30,   AUC: 0.8357077543409182,   Loss Mean: 0.5576604008674622\n","Epoch: 31,   AUC: 0.8357541540085471,   Loss Mean: 0.5569449663162231\n","Epoch: 32,   AUC: 0.8346466561812147,   Loss Mean: 0.5580413341522217\n","Epoch: 33,   AUC: 0.8341250389453638,   Loss Mean: 0.5564226508140564\n","Epoch: 34,   AUC: 0.8364861778662456,   Loss Mean: 0.5571121573448181\n","Epoch: 35,   AUC: 0.8350836312250536,   Loss Mean: 0.5563796162605286\n","Epoch: 36,   AUC: 0.83482662512588,   Loss Mean: 0.5575491189956665\n","Epoch: 37,   AUC: 0.8346233836096596,   Loss Mean: 0.5561341047286987\n","Epoch: 38,   AUC: 0.8343368757839902,   Loss Mean: 0.5572482347488403\n","Epoch: 39,   AUC: 0.8341243203315412,   Loss Mean: 0.5548263192176819\n","Epoch: 40,   AUC: 0.8344874691977577,   Loss Mean: 0.555696427822113\n","Epoch: 41,   AUC: 0.8337872518889378,   Loss Mean: 0.5558910369873047\n","Epoch: 42,   AUC: 0.8342001060463676,   Loss Mean: 0.5542806386947632\n","Epoch: 43,   AUC: 0.8338540444146809,   Loss Mean: 0.5557229518890381\n","Epoch: 44,   AUC: 0.833737515048825,   Loss Mean: 0.5553063750267029\n","Epoch: 45,   AUC: 0.8323200282509879,   Loss Mean: 0.5548822283744812\n","Epoch: 46,   AUC: 0.8335733889098746,   Loss Mean: 0.5549191832542419\n","Epoch: 47,   AUC: 0.8337819516738163,   Loss Mean: 0.5537111759185791\n","Epoch: 48,   AUC: 0.8325560525792558,   Loss Mean: 0.5545706748962402\n","Epoch: 49,   AUC: 0.8324860806255263,   Loss Mean: 0.5544041991233826\n","Epoch: 50,   AUC: 0.8321047420660126,   Loss Mean: 0.5536344051361084\n","Epoch: 51,   AUC: 0.8314365241049017,   Loss Mean: 0.554416835308075\n","Epoch: 52,   AUC: 0.8319165844292018,   Loss Mean: 0.5528022050857544\n","Epoch: 53,   AUC: 0.832400977469038,   Loss Mean: 0.5530340671539307\n","Epoch: 54,   AUC: 0.8322754110961825,   Loss Mean: 0.552875816822052\n","Epoch: 55,   AUC: 0.8317444904404031,   Loss Mean: 0.5529287457466125\n","Epoch: 56,   AUC: 0.8302988409915832,   Loss Mean: 0.5528666377067566\n","Epoch: 57,   AUC: 0.8295958894448039,   Loss Mean: 0.5530310273170471\n","Epoch: 58,   AUC: 0.8311314462464241,   Loss Mean: 0.5521568059921265\n","Epoch: 59,   AUC: 0.8304562419567834,   Loss Mean: 0.5519131422042847\n","Epoch: 60,   AUC: 0.8305832192665343,   Loss Mean: 0.5522029399871826\n","Epoch: 61,   AUC: 0.8307332693381433,   Loss Mean: 0.5522487759590149\n","Epoch: 62,   AUC: 0.8298188279906324,   Loss Mean: 0.5522264838218689\n","Epoch: 63,   AUC: 0.8294562557681904,   Loss Mean: 0.5517579317092896\n","Epoch: 64,   AUC: 0.8300044161448479,   Loss Mean: 0.5514037013053894\n","Epoch: 65,   AUC: 0.8302108966814202,   Loss Mean: 0.5513755083084106\n","Epoch: 66,   AUC: 0.8294368777330112,   Loss Mean: 0.5500548481941223\n","Epoch: 67,   AUC: 0.8290711453624661,   Loss Mean: 0.5504643321037292\n","Epoch: 68,   AUC: 0.828811965894658,   Loss Mean: 0.5510594844818115\n","Epoch: 69,   AUC: 0.828801922828307,   Loss Mean: 0.5505816340446472\n","Epoch: 70,   AUC: 0.828922036499693,   Loss Mean: 0.5502484440803528\n","Epoch: 71,   AUC: 0.8291431434566321,   Loss Mean: 0.550473690032959\n","Epoch: 72,   AUC: 0.8281906313509687,   Loss Mean: 0.549454927444458\n","Epoch: 73,   AUC: 0.8278017280664338,   Loss Mean: 0.5502743721008301\n","Epoch: 74,   AUC: 0.8271260260052672,   Loss Mean: 0.5511696934700012\n","Epoch: 75,   AUC: 0.8279553028512072,   Loss Mean: 0.5499899387359619\n","Epoch: 76,   AUC: 0.8278420738507857,   Loss Mean: 0.5502620935440063\n","Epoch: 77,   AUC: 0.8275720853794548,   Loss Mean: 0.5481593608856201\n","Epoch: 78,   AUC: 0.8266897941132838,   Loss Mean: 0.5491679310798645\n","Epoch: 79,   AUC: 0.8264004959627222,   Loss Mean: 0.5482037663459778\n","Epoch: 80,   AUC: 0.8258421908621487,   Loss Mean: 0.5487240552902222\n","Epoch: 81,   AUC: 0.8277818732923282,   Loss Mean: 0.5492109656333923\n","Epoch: 82,   AUC: 0.8262010105182629,   Loss Mean: 0.5482712984085083\n","Epoch: 83,   AUC: 0.8275877739461055,   Loss Mean: 0.5491839647293091\n","Epoch: 84,   AUC: 0.8261599058076056,   Loss Mean: 0.5483952760696411\n","Epoch: 85,   AUC: 0.8252264317100934,   Loss Mean: 0.5478399991989136\n","Epoch: 86,   AUC: 0.8261183261107385,   Loss Mean: 0.547465980052948\n","Epoch: 87,   AUC: 0.825237189884834,   Loss Mean: 0.5482122302055359\n","Epoch: 88,   AUC: 0.8259218448226013,   Loss Mean: 0.5481722354888916\n","Epoch: 89,   AUC: 0.8260007135659988,   Loss Mean: 0.5477185845375061\n","Epoch: 90,   AUC: 0.825260432660206,   Loss Mean: 0.547601044178009\n","Epoch: 91,   AUC: 0.826384581295625,   Loss Mean: 0.5475375056266785\n","Epoch: 92,   AUC: 0.8253952901839735,   Loss Mean: 0.5469586253166199\n","Epoch: 93,   AUC: 0.8243455513808182,   Loss Mean: 0.5465888381004333\n","Epoch: 94,   AUC: 0.8240914022097691,   Loss Mean: 0.5469765067100525\n","Epoch: 95,   AUC: 0.8249341258977765,   Loss Mean: 0.5474329590797424\n","Epoch: 96,   AUC: 0.8246508308017095,   Loss Mean: 0.5461737513542175\n","Epoch: 97,   AUC: 0.824282339655172,   Loss Mean: 0.5475462675094604\n","Epoch: 98,   AUC: 0.825964244790856,   Loss Mean: 0.5465094447135925\n","Epoch: 99,   AUC: 0.8245684846684969,   Loss Mean: 0.546764075756073\n","Epoch: 100,   AUC: 0.824647037922899,   Loss Mean: 0.5467392802238464\n"]}]}]}