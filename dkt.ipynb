{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dkt.ipynb","provenance":[],"authorship_tag":"ABX9TyOdAVOlcK7rmAyCFt3g2yW3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","%cd drive/MyDrive/Colab Notebooks/knowledge-tracing"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZzF7pAqiNrO9","executionInfo":{"status":"ok","timestamp":1646493054808,"user_tz":-540,"elapsed":2116,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}},"outputId":"4ca02f15-0034-4aa5-c020-2f4bffcf9c49"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","[Errno 2] No such file or directory: 'drive/MyDrive/Colab Notebooks/knowledge-tracing'\n","/content/drive/MyDrive/Colab Notebooks/knowledge-tracing\n"]}]},{"cell_type":"code","execution_count":97,"metadata":{"id":"BVDNwmhXE-eE","executionInfo":{"status":"ok","timestamp":1646493054809,"user_tz":-540,"elapsed":8,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"outputs":[],"source":["import os\n","import argparse\n","import json\n","import pickle\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","from torch.utils.data import DataLoader, random_split\n","from torch.optim import SGD, Adam\n","from torch.utils.data import Dataset\n","\n","from torch.nn import Module, Embedding, LSTM, Linear, Dropout\n","from torch.nn.functional import one_hot, binary_cross_entropy\n","from sklearn import metrics\n","from torch.nn.utils.rnn import pad_sequence"]},{"cell_type":"code","source":["DATASET_DIR = \"./data/assist2009/\""],"metadata":{"id":"q-rZYJ_WZHla","executionInfo":{"status":"ok","timestamp":1646493054809,"user_tz":-540,"elapsed":6,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":98,"outputs":[]},{"cell_type":"code","source":["def match_seq_len(q_seqs, r_seqs, seq_len, pad_val=-1):\n","    '''\n","        Args:\n","            q_seqs: the question(KC) sequences with the size of \\\n","                [batch_size, some_sequence_length]\n","            r_seqs: the response sequences with the size of \\\n","                [batch_size, some_sequence_length]\n","            Note that the \"some_sequence_length\" is not uniform over \\\n","                the whole batch of q_seqs and r_seqs\n","            seq_len: the sequence length to match the q_seqs, r_seqs \\\n","                to same length\n","            pad_val: the padding value for the sequence with the length \\\n","                longer than seq_len\n","        Returns:\n","            proc_q_seqs: the processed q_seqs with the size of \\\n","                [batch_size, seq_len + 1]\n","            proc_r_seqs: the processed r_seqs with the size of \\\n","                [batch_size, seq_len + 1]\n","    '''\n","    proc_q_seqs = []\n","    proc_r_seqs = []\n","\n","    for q_seq, r_seq in zip(q_seqs, r_seqs):\n","        i = 0\n","        while i + seq_len + 1 < len(q_seq):\n","            proc_q_seqs.append(q_seq[i:i + seq_len + 1])\n","            proc_r_seqs.append(r_seq[i:i + seq_len + 1])\n","\n","            i += seq_len + 1\n","\n","        proc_q_seqs.append(\n","            np.concatenate(\n","                [\n","                    q_seq[i:],\n","                    np.array([pad_val] * (i + seq_len + 1 - len(q_seq)))\n","                ]\n","            )\n","        )\n","        proc_r_seqs.append(\n","            np.concatenate(\n","                [\n","                    r_seq[i:],\n","                    np.array([pad_val] * (i + seq_len + 1 - len(q_seq)))\n","                ]\n","            )\n","        )\n","\n","    return proc_q_seqs, proc_r_seqs"],"metadata":{"id":"TyFBEpINcBdi","executionInfo":{"status":"ok","timestamp":1646493054810,"user_tz":-540,"elapsed":7,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":99,"outputs":[]},{"cell_type":"code","source":["class ASSIST2009(Dataset):\n","    def __init__(self, seq_len, dataset_dir=DATASET_DIR) -> None:\n","        super().__init__()\n","\n","        self.dataset_dir = dataset_dir\n","        self.dataset_path = os.path.join(\n","            self.dataset_dir, \"skill_builder_data.csv\"\n","        )\n","\n","        if os.path.exists(os.path.join(self.dataset_dir, \"q_seqs.pkl\")):\n","            with open(os.path.join(self.dataset_dir, \"q_seqs.pkl\"), \"rb\") as f:\n","                self.q_seqs = pickle.load(f)\n","            with open(os.path.join(self.dataset_dir, \"r_seqs.pkl\"), \"rb\") as f:\n","                self.r_seqs = pickle.load(f)\n","            with open(os.path.join(self.dataset_dir, \"q_list.pkl\"), \"rb\") as f:\n","                self.q_list = pickle.load(f)\n","            with open(os.path.join(self.dataset_dir, \"u_list.pkl\"), \"rb\") as f:\n","                self.u_list = pickle.load(f)\n","            with open(os.path.join(self.dataset_dir, \"q2idx.pkl\"), \"rb\") as f:\n","                self.q2idx = pickle.load(f)\n","            with open(os.path.join(self.dataset_dir, \"u2idx.pkl\"), \"rb\") as f:\n","                self.u2idx = pickle.load(f)\n","        else:\n","            self.q_seqs, self.r_seqs, self.q_list, self.u_list, self.q2idx, \\\n","                self.u2idx = self.preprocess()\n","\n","        self.num_u = self.u_list.shape[0]\n","        self.num_q = self.q_list.shape[0]\n","\n","        if seq_len:\n","            self.q_seqs, self.r_seqs = \\\n","                match_seq_len(self.q_seqs, self.r_seqs, seq_len)\n","\n","        self.len = len(self.q_seqs)\n","\n","    def __getitem__(self, index):\n","        return self.q_seqs[index], self.r_seqs[index]\n","\n","    def __len__(self):\n","        return self.len\n","\n","    def preprocess(self):\n","        df = pd.read_csv(self.dataset_path).dropna(subset=[\"skill_name\"])\\\n","            .drop_duplicates(subset=[\"order_id\", \"skill_name\"])\\\n","            .sort_values(by=[\"order_id\"])\n","\n","        u_list = np.unique(df[\"user_id\"].values)\n","        q_list = np.unique(df[\"skill_name\"].values)\n","\n","        u2idx = {u: idx for idx, u in enumerate(u_list)}\n","        q2idx = {q: idx for idx, q in enumerate(q_list)}\n","\n","        q_seqs = []\n","        r_seqs = []\n","\n","        for u in u_list:\n","            df_u = df[df[\"user_id\"] == u]\n","\n","            q_seq = np.array([q2idx[q] for q in df_u[\"skill_name\"]])\n","            r_seq = df_u[\"correct\"].values\n","\n","            q_seqs.append(q_seq)\n","            r_seqs.append(r_seq)\n","\n","        with open(os.path.join(self.dataset_dir, \"q_seqs.pkl\"), \"wb\") as f:\n","            pickle.dump(q_seqs, f)\n","        with open(os.path.join(self.dataset_dir, \"r_seqs.pkl\"), \"wb\") as f:\n","            pickle.dump(r_seqs, f)\n","        with open(os.path.join(self.dataset_dir, \"q_list.pkl\"), \"wb\") as f:\n","            pickle.dump(q_list, f)\n","        with open(os.path.join(self.dataset_dir, \"u_list.pkl\"), \"wb\") as f:\n","            pickle.dump(u_list, f)\n","        with open(os.path.join(self.dataset_dir, \"q2idx.pkl\"), \"wb\") as f:\n","            pickle.dump(q2idx, f)\n","        with open(os.path.join(self.dataset_dir, \"u2idx.pkl\"), \"wb\") as f:\n","            pickle.dump(u2idx, f)\n","\n","        return q_seqs, r_seqs, q_list, u_list, q2idx, u2idx"],"metadata":{"id":"IrShEGvnY8EI","executionInfo":{"status":"ok","timestamp":1646493131317,"user_tz":-540,"elapsed":273,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":104,"outputs":[]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    from torch.cuda import FloatTensor\n","    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n","else:\n","    from torch import FloatTensor"],"metadata":{"id":"9H4cbWs8hwoU","executionInfo":{"status":"ok","timestamp":1646493136840,"user_tz":-540,"elapsed":265,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":105,"outputs":[]},{"cell_type":"code","source":["class DKT(Module):\n","    '''\n","        Args:\n","            num_q: the total number of the questions(KCs) in the given dataset\n","            emb_size: the dimension of the embedding vectors in this model\n","            hidden_size: the dimension of the hidden vectors in this model\n","    '''\n","    \n","    def __init__(self, num_q, emb_size, hidden_size):\n","        super().__init__()\n","        self.num_q = num_q\n","        self.emb_size = emb_size\n","        self.hidden_size = hidden_size\n","\n","        self.interaction_emb = Embedding(self.num_q * 2, self.emb_size)\n","        self.lstm_layer = LSTM(\n","            self.emb_size, self.hidden_size, batch_first=True\n","        )\n","        self.out_layer = Linear(self.hidden_size, self.num_q)\n","        self.dropout_layer = Dropout()\n","\n","    def forward(self, q, r):\n","        '''\n","            Args:\n","                q: the question(KC) sequence with the size of [batch_size, n]\n","                r: the response sequence with the size of [batch_size, n]\n","            Returns:\n","                y: the knowledge level about the all questions(KCs)\n","        '''\n","        x = q + self.num_q * r\n","\n","        h, _ = self.lstm_layer(self.interaction_emb(x))\n","        y = self.out_layer(h)\n","        y = self.dropout_layer(y)\n","        y = torch.sigmoid(y)\n","\n","        return y\n","\n","    def train_model(\n","        self, train_loader, test_loader, num_epochs, opt, ckpt_path\n","    ):\n","        '''\n","            Args:\n","                train_loader: the PyTorch DataLoader instance for training\n","                test_loader: the PyTorch DataLoader instance for test\n","                num_epochs: the number of epochs\n","                opt: the optimization to train this model\n","                ckpt_path: the path to save this model's parameters\n","        '''\n","        aucs = []\n","        loss_means = []\n","\n","        max_auc = 0\n","\n","        for i in range(1, num_epochs + 1):\n","            loss_mean = []\n","\n","            for data in train_loader:\n","                q, r, qshft, rshft, m = data\n","\n","                self.train()\n","\n","                y = self(q.long(), r.long())\n","                y = (y * one_hot(qshft.long(), self.num_q)).sum(-1)\n","\n","                y = torch.masked_select(y, m)\n","                t = torch.masked_select(rshft, m)\n","\n","                opt.zero_grad()\n","                loss = binary_cross_entropy(y, t)\n","                loss.backward()\n","                opt.step()\n","\n","                loss_mean.append(loss.detach().cpu().numpy())\n","\n","            with torch.no_grad():\n","                for data in test_loader:\n","                    q, r, qshft, rshft, m = data\n","\n","                    self.eval()\n","\n","                    y = self(q.long(), r.long())\n","                    y = (y * one_hot(qshft.long(), self.num_q)).sum(-1)\n","\n","                    y = torch.masked_select(y, m).detach().cpu()\n","                    t = torch.masked_select(rshft, m).detach().cpu()\n","\n","                    auc = metrics.roc_auc_score(\n","                        y_true=t.numpy(), y_score=y.numpy()\n","                    )\n","\n","                    loss_mean = np.mean(loss_mean)\n","\n","                    print(\n","                        \"Epoch: {},   AUC: {},   Loss Mean: {}\"\n","                        .format(i, auc, loss_mean)\n","                    )\n","\n","                    if auc > max_auc:\n","                        torch.save(\n","                            self.state_dict(),\n","                            os.path.join(\n","                                ckpt_path, \"model.ckpt\"\n","                            )\n","                        )\n","                        max_auc = auc\n","\n","                    aucs.append(auc)\n","                    loss_means.append(loss_mean)\n","\n","        return aucs, loss_means"],"metadata":{"id":"-480DfL6JwoW","executionInfo":{"status":"ok","timestamp":1646493137981,"user_tz":-540,"elapsed":2,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":106,"outputs":[]},{"cell_type":"code","source":["dataset = ASSIST2009(5)"],"metadata":{"id":"ucGbzSvGjmn6","executionInfo":{"status":"ok","timestamp":1646493141024,"user_tz":-540,"elapsed":290,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"execution_count":107,"outputs":[]}]}